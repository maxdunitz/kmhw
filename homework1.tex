% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework}%replace X with the appropriate number
\author{Max Dunitz\\ %replace with your name
Kernel Methods} %if necessary, replace with your course title
 
\maketitle


\begin{exercise}{1} 
	\begin{enumerate}
		\item The symmetry of the kernel $K(x,y) = \cos (x-y) = \cos (y-x) = K(y,x)$ follows immediately from the evenness of the cosine function. The positive definiteness can be seen by noting that $K(x,y) = \cos(x-y) = \frac{1}{2} e^{j(x-y)} + \frac{1}{2} e^{-j(x-y)}$, a nonnegative linear combination of two exponential kernels. The positive definiteness of the latter is established by the following: For any real collection of $\alpha_i$, $\sum_i \sum_k \alpha_i \alpha_k e^{j(x_i - y_k)} = \big(\sum_i \alpha_i e^{jx_i}\big)\big(\sum_k \alpha_k e^{-j x_i} \big).$ Letting $s=\sum_i \alpha_i e^{jx_i}$, this becomes $s \cdot \bar{s} = |s|^2 \geq 0$, thus establishing the positive definiteness of $K_{exp} = e^{j(x-y)}$. The argument for the positive definiteness of $K_{exp-conjugate} = e^{-j(x-y)}$ holds by symmetry. Thus this cosine kernel, as the nonnegative combination of two positive definite kernels, is positive definite.
		\item The symmetry of the kernel $K(x,y) = \frac{1}{1-x^Ty} = \frac{1}{1-y^Tx} = K(y,x)$ follows immediately from the symmetry of the linear kernel $K_{lin}(x,y) = x^Ty$. Since $\mathcal{X}$ contains only those vectors $x \in \mathbb{R}^p$ for which $||x||_2 < 1$, by the Cauchy-Schwartz inequality we have that for $x,y \in \mathcal{X}, |x^T y| < 1$ and so $K(x,y)$ may be expressed as the convergent infinite series $1 + (x^T y) + (x^T y)^2 + (x^T y)^3 + \ldots.$ We can thus define a sequence of positive definite kernels $K_i(x,y) = \sum_{k=0}^i (x^Ty)^k$ for $i >0$ that converges pointwise to $K(x,y) = \frac{1}{1-x^Ty}$. Each partial sum in the sequence is positive definite as it is a non-weighted sum of positive definite (polynomial) kernels. But there is a slight complication: the zeroth-order kernel $K(x,y) = 1$ is only positive semidefinite, not positive definite. That is, the rank-one Gram matrix associated with the symmetric ``semidefinite kernel" $K(x,y) = 1$---ie, the all-ones matrix---has one positive eigenvalue but the rest are zero. This is not a concern as the sum of a positive definite matrix with that of a positive semidefinite matrix is positive definite: if $x^TAx > 0$ and $x^TBx \geq 0$ then $x^T(A+B)x = x^TAx + x^T B x > 0.$
		\item Since $P(A \cap B) = P(B \cap A)$ and $P(A)P(B) = P(B)P(A)$, the kernel is symmetric. Given a set $A$ define the indicator function $I_A: \mathbb{R} \rightarrow \mathbb{R}$ that maps $x$ to 1 if $x\in A$ and 0 otherwise. This function is measurable when $A$ is measurable with respect to a probability measure. Let $\mu_A \triangleq \mathbb{E}[I_A] = \int I_A (x) d \mu (x) = \int_{x \in A} d \mu (x) = P(A).$ Now define the function $\phi_A(x) = I_A(x) - \mu_A.$ We can repeat this process, making equivalent definitions of indicator function, mean, and difference function $\phi$, for any set---and use them to rewrite our kernel as an inner product: $K(A,B) = P(A\cap B) - P(A)P(B) = \int I_A (x) I_B(x) d \mu(x) - \big(\int I_A(x) d \mu(x) \big) \cdot \big(\int I_B(x) d\mu(x)\big) = \mathbb{E}[I_A I_B] - \mathbb{E}[I_A]\mathbb{E}[I_B] =\mathbb{E}[I_AI_B] - \mu_A\mu_B = \mathbb{E}[I_AI_B] - \mu_A\mathbb{E}[I_B] - \mu_B \mathbb{E}[I_A] -\mu_A\mu_B = \mathbb{E}[(I_A -\mu_A)(I_B - \mu_B)] = \int \phi_A (x) \phi_B (x) d \mu (x) = \langle \phi_A, \phi_B \rangle_{L_2(\mu)}$. Having found an appropriate inner product, we can apply the Aronsajn theorem, thereby establishing the positive definiteness of $K(A,B)$. We note that for all $x\in \mathbb{R}$, we have that $I_A(x)I_B(x) \leq I_A(x)$ and $\int I_A(x) \cdot \mu_B d\mu(x) = \mu_B \cdot \int I_A(x) d\mu(x)$ is finite since $A$ an $B$ are measurable sets (and hence $\mu_A$ and $\mu_B$ are finite), this is all good and kosher.
		\item We note that $K_4(x,y) = \min \big(f(x)g(y), f(y)g(x)\big) = \min \big(f(y)g(x), f(x)g(y)\big) = K_4(y,x)$, where the second equality holds by the symmetry of the min function. Thus $K_4$ is symmetric. To see that this is a positive definite kernel, let us first factor out $f(x)f(y)$ to rewrite $K_4(x,y) = f(x)f(y)\min\big(\frac{g(x)}{f(x)}, \frac{g(y)}{f(y)}\big)$. We can do this since $f$ is positive. Now let the function $\mathbf{1}_a$ map reals between 0 and $a$ to 1 and all other reals to 0. As we saw in class, $K_a(x,y) = \min\big(\frac{g(x)}{f(x)}, \frac{g(y)}{f(y)}\big)$ is a positive definite kernel, which we can see by finding an inner product and applying Aronszajn's theorem or directly from the definition: since for any choice of $n$ vectors $x_1, \ldots , x_n$ in $\mathcal{X}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$, we have that 
			\begin{align*} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \min\bigg(\frac{g(x_i)}{f(x_i)}, \frac{g(x_j)}{f(x_j)}\bigg) &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_{0}^\infty \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \mathbf{1}_{\frac{g(x_j)}{f(x_j)}}\\ &= \int_{0}^\infty  \bigg(\sum_{i=1}^n \alpha_i \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \bigg) \bigg(\sum_{j=1}^n \alpha_j \mathbf{1}_{\frac{g(x_j)}{f(x_j)}}\bigg)\\ &=  \int_{0}^\infty  \big(\sum_{i=1}^n \alpha_i \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \big)^2 \geq 0,\end{align*} we see that $K_a$ is positive definite. Similarly, $K_b(x,y) = f(x)f(y)$ is positive definite since for any choice of $n$ vectors $x_1, \ldots , x_n$ in $\mathcal{X}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$, we have that \begin{align*}\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j f(x_i)f(x_j) &= \bigg(\sum_{i=1}^n \alpha_i f(x_i)\bigg)^2 \geq 0.\end{align*} Since $K_4(x,y) = K_a(x,y)K_b(x,y)$ is the product of two positive definite kernels it must itself be positive definite. (See slide 48 for proof.)
			\item The symmetry of the kernel follows immediately from the facts that $A \cup B = B \cup A$ and $A \cap B = B \cap A$. Note that $|A \cap B| = \sum_{x\in E} I_x(A) \cdot I_x(B)$, where the indicator $I_x(C): \mathcal{P} \rightarrow \mathbb{R}$ takes the value 1 if $x \in C$ and 0 otherwise. We may also write $|A \cup B| = |E| - |\bar{A} \cap \bar{B}|,$ where for any set $C$, $\bar{C}$ indicates its set complement. We will let $K_1(A,B) = |A \cap B|$ and $K_2(A,B) = \frac{1}{|A \cup B|} = \frac{1}{|E| - |\bar{A} \cap \bar{B}|}$ and show that $K(A,B) = K_1(A,B) \cdot K_2(A,B)$ is the product of two positive definite kernels. Let us index the $n=|E|$ elements of the set $E$ as follows: $\{x_i | i \in I \}$  with the index set $I=\{1,\ldots, n\}$. Consider the feature map $\phi(C): \mathcal{P}{E} \rightarrow \mathbb{R}^{n} = (I_{x_1}(C), \ldots, I_{x_n}(C))^T$. Then $K_1(A,B) = \langle \phi(A), \phi(B) \rangle$ where the inner product used is the standard inner product used for $\mathbb{R}$. Hence by Aronszajn's theorem it is positive definite. Now let $\psi(C): \mathcal{P}(E) \rightarrow \mathbb{R}^{n} = \frac{1}{\sqrt{n}}\cdot (\bar{I}_{x_1}(C), \ldots, \bar{I}_{x_n}(C))^T,$ where $\bar{I}_{x}(C): \mathcal{P}(E) \rightarrow \mathbb{R}$ takes the value 0 if $x \in C$ and 1 otherwise. Then $K_2(A,B) = \frac{1}{n - n \psi(A)^T\psi(B)} = \frac{1}{n} \cdot \frac{1}{1- \psi(A)^T\psi(B)}$ and we can apply Aronszajn's theorem, exercise 1-2, and the property that nonnegative scalar multiples of positie definite kernels are positive definite (see slide 46). Thus $K$ is the product of two positie definite kernels and is positive definite by the result shown in class.
				
				%We saw in class that $K_{min-over-max}(x,y) = \frac{\min(x,y)}{\max(x,y)}$ is a positive definite kernel (it is the product of the two positive definite kernels $\min(x,y)$ and $\big(\frac{1}{\min(1/x, 1/y)}\big)$. Let $I_A(x)$ be the indicator for $A$ (ie, it equals 1 if $x \in A$ and 0 otherwise) and $I_B(x)$ the indicator for $B$. Let $f_x$ map a set $BQH$.Then $K(A,B) = \sum_{x \in E} \frac{|A \cap B|}{|A \cup B|} = \sum_{x \in E} \frac{\min(I_A(x), I_B(x))}{\max(I_A(x), I_B(x))}$ is a nonnegative weighted sum of positive definite kernels and is therefore positive definite.
	\end{enumerate}
\end{exercise}

\section{Exercise 2}
\subsection{Problem 1}
	Let $K \triangleq \alpha K_1 + \beta K_2$. That $K$ is symmetric follows immediately from the symmetry of $K_1$ and $K_2$: for some $x, y \in \mathcal{X}, K(x,y) = \alpha K_1(x,y) + \beta K_2(x,y) = \alpha K_1(y,x) + \beta K_2(y,x) = K(y,x)$. That $K$ is positive definite comes almost as easily: for any set of data observations $\{x_i | x_i \in \mathcal{X}, i=1,\ldots,n\}$ and corresponding real weights $\alpha_i$, we have that for $K_j \in \{K_1, K_2 \}, \sum_{i=1}^n \sum_{l=1}^n \alpha_i \alpha_l K_j(x_i, x_l) \geq 0$---and since $\alpha, \beta > 0$, we see that \begin{align*}\sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K(x_i, x_l) &= \sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l \alpha K_1(x_i, x_l) + \beta K_2(x_i, x_l)\\ &= \alpha \cdot \big( \sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K_1(x_i, x_l)\big) + \beta \cdot \big(\sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K_2(x_i, x_l)\big)\\ &\geq \alpha \cdot 0 + \beta \cdot 0 =  0.\end{align*} Thus $K$ is positive definite.

		(We may also view this result by applying the positive definiteness of the constituent kernels to write $K_1(x,y) = \langle \phi(x), \phi(y) \rangle_{\mathcal{H}_1}$ and $K_2(x,y) = \langle \psi(x), \psi(y) \rangle_{\mathcal{H}_2}$. Then we can find a feature vector $\Phi(x) = [\sqrt{\alpha}\phi(x), \sqrt{\beta}\psi(x)]$ that is a concatenation of scaled versions of the constituent feature vectors. Then $K(x,y) = \Phi(x)^T\Phi(y)$ and we may apply Aronzjan's thereom.)  	

	Now we move onto the problem of a description of the RKHS (that's more satisfying than ``take the span of the feature vectors for each point in $\mathcal{X}$ an complete the space").


	%We can view $\mathcal{H} \triangleq \mathcal{H}_1 + \mathcal{H}_2 = \{ax + by | x \in \mathcal{H}_1, y \in \mathcal{H}_2, a,b\in \mathbb{R}\} = \mathcal{H}_1 \bigoplus \mathcal{H}_1^\bot = \mathcal{H}_2 \bigoplus \mathcal{H}_2^\bot$. We know for all $x\in \mathcal{X}$, there exist $K_x^1 \in \mathcal{H}_1$ and $K_x^2 \in \mathcal{H}_2$ such that for all $f \in \mathcal{H}_1, f(x) = \langle f, K_x^1 \rangle_{\mathcal{H}_1}$ and the equivalent result holds for all $f \in \mathcal{H_2}$. For any $f \in \mathcal{H}$ we can decompose $f$ as the sum of an element of $\mathcal{H}_1$ or $\mathcal{H}_2$ and its respective orthogonal complement: $f = f_1 + f_1^\bot$ and  $f = f_2 + f_2^\bot$. We have 


We consider two Hilbert spaces $\mathcal{F} = \mathcal{H}_1 \otimes \mathcal{H}_2 = \{(f,g) | f \in \mathcal{H}_1, g \in \mathcal{H}_2\}$ and $\mathcal{H} = \mathcal{H}_1 + \mathcal{H}_2 = \{af+bg | f  \in \mathcal{H}_1, g \in \mathcal{H}_2, a,b\in \mathbb{R}\}$ as well as the mapping $\phi: \mathcal{F} \rightarrow \mathcal{H}$ characterized by $\phi((f,g)) = f+g$. Let us note that $\mathcal{F} = \text{null}(\phi) \oplus \text{null}(\phi)^\bot$. Suppose some $f \in {H}$ can be decomposed two ways as a sum of elements of $\mathcal{H}_1$ and $\mathcal{H}_2$. Then $f = f_1 + f_2 = f_3 + f_4$ and $(f_3-f_1, f_4-f_2)$ is in $\text{null}(\phi)$. Suppose $g \in \text{null}(\phi)^\bot$. Then $\langle g, (f_3-f_1, f_4-f_2)\rangle_\mathcal{F} = 0 \implies \langle g, (f_1, f_2)\rangle_\mathcal{F} = \langle g, (f_3, f_4) \rangle_\mathcal{F}$ so the inner product between an element $f$ of $\text{null}(\phi)^\bot$ and an arbitary element of $\mathcal{F}$ depends only on the sum of the elements of $f$'s decomposition into a sum of elements of $\mathcal{H}_1$ and $\mathcal{H}_2$ and not the decomposition itself. Let the projection of some $(h_1, h_2) \in \mathcal{F}$ into null($\phi^\bot$) be denoted $(h_1^\bot, h_2^\bot)$. Given the inner products of the two RKHSs $\mathcal{H}_1$ an $\mathcal{H}_2$ of the kernels $K_1$ and $K_2$, consider the following inner product between two arbitrary elements $f = f_1+f_2$ and $g=g_1+g_2$ of $\mathcal{H}$:
$$ \langle f, g \rangle_\mathcal{H} = \frac{1}{\alpha} \langle f_1^\bot, g_1^\bot \rangle_{\mathcal{H}_1} + \frac{1}{\beta} \langle f_2^\bot, g_2^\bot \rangle_{\mathcal{H}_2}$$. 
It is obviously symmetric, bilinear, and positive as the two inner products used in the definiton have that property. That it's definite follows from the positive definiteness of the constituent inner products: $\langle f, f \rangle_\mathcal{H} = \frac{1}{\alpha} \langle f_1^\bot, f_1^\bot \rangle_{\mathcal{H}_1} + \frac{1}{\beta} \langle f_2^\bot, f_2^\bot \rangle_{\mathcal{H}_2} \implies f_1^\bot = 0 $ and $f_2^\bot = 0 \implies (f_1, f_2) \in $ null($\phi) \implies f_1 + f_2 = f = 0.$

We verify that the reproducing property holds. For some $x \in \mathcal{X}$, let $K_{x1} \in \mathcal{H}_1$ be the reproducing kernel $K_1(x, \cdot)$ and $K_{x2} \in \mathcal{H}_2$ the rk of $K_2(x, \cdot)$. Let us define, for this same $x\in \mathcal{X}$, $K_x = \alpha K_{x1} + \beta K_{x2} \in \mathcal{H}$. Consider some $f \in \mathcal{X}$, which can be decomposed (arbitrarily) into $f_1 \in \mathcal{H}_1$ and $f_2 \in \mathcal{H}_2$. Then we have that

\begin{align*} 
	\langle f, K_x \rangle_{\mathcal{H}} &= \langle (f_1^\bot, f_2^\bot), (\alpha K_{x1}^\bot, \beta K_{x2}^\bot) \rangle_{\mathcal{F}}\\
	&= \langle (f_1^\bot, f_2^\bot), (\alpha K_{x1}^\bot, \beta K_{x2}^\bot) \rangle_{\mathcal{F}} + 0 \\
	&= \langle (f_1^\bot, f_2^\bot), (\alpha K_{x1}^\bot, \beta K_{x2}^\bot) \rangle_{\mathcal{F}} + 
	\langle (f_1^\bot, f_2^\bot), (\alpha K_{x1} - \alpha K_{x1}^\bot, \beta K_{x2} - \beta K_{x2}^\bot) \rangle_{\mathcal{F}}\\
	&= \langle (f_1^\bot, f_2^\bot), (\alpha K_{x1}, \beta K_{x2}) \rangle_{\mathcal{F}}\\
	&= \frac{1}{\alpha} \langle f_1^\bot,  \alpha K_{x1} \rangle_{\mathcal{H}_1} + \frac{1}{\beta} \langle f_2^\bot, \beta K_{x2}\rangle_{\mathcal{H}_2}\\
	&= f_1^\bot(x) + f_2^\bot(x) = f_1(x) + f_2(x) = f(x),
\end{align*} where we have used the linearity of the projection operator, the orthogonality between a vector in a subspace and one in its orthogonal complement, bilinearity, the definition of $\langle \cdot, \cdot \rangle_\mathcal{H}$ given above, and the fact that $f_1+f_2 = \phi(f) = \phi((f_1, f_2)) = \phi((f_1^\bot, f_2^\bot)) + \phi(f_1^{\text{null}}, f_2^{\text{null}}) = f_1^\bot + f_2^\bot + (f_1^{\text{null}}, f_2^{\text{null}}) = f_1^\bot + f_2^\bot + 0 = f_1^\bot + f_2^\bot.$ Hence, the reproducing property holds.

The completeness of $\mathcal{F}$ comes immediately from the tensor product property and the completeness of the constituent spaces. Since we know by the Cauchy-Schwarz inequality that the orthogonal projection map $(f_1, f_2) \rightarrow (f_1^\bot, f_2^\bot)$ is linear and bounded and hence continuous\footnote{$||P_\bot u||^2 = \langle Pu, Pu\rangle = \langle Pu, u \rangle \leq ||Pv|| \cdot ||v|| \implies ||Pv|| \leq ||v||$. We noted that $0 = \langle Pv, v - Pv\rangle \implies \langle Pv, v\rangle = \langle Pv, Pv\rangle$}, and since we know the subpsaces $\mathcal{H}_1$ and $\mathcal{H}_2$ are complete, we can construct a Cauchy sequence of elements of $\mathcal{H}$ that can be decomposed into corresponding Cauchy sequences in those two subpsaces an from that we can show that under the corresponding norm $||f||_\mathcal{H} = \sqrt{||\frac{1}{\alpha} f_1^\bot||^2 + ||\frac{1}{\beta}f_2^\bot||^2}$, we have that $\mathcal{H}$ is a (complete) Hilbert space.



%	We now consider what $K$'s RKHS is. If $\mathcal{H}_1$ is the RKHS associated with $K_1$ an $\mathcal{H}_2$ the RKHS associated with $K_2$ are both subspaces of the same Hilbert space, over the same field $\mathcal{F}$ with the same norm $|| \cdot ||$, define $\mathcal{H} \triangleq \mathcal{H}_1 + \mathcal{H}_2 = \{a_1 x_1 + a_2 x_2 | x_1 \in \mathcal{H}_1, x_2 \in \mathcal{H}_2, a_1,a_2 \in \mathcal{F}\}$. Clearly any sequence in $\mathcal{H}$ that is Cauchy with respect to the norm $|| \cdot ||_\mathcal{H}$  will converge since each element of $\mathcal{H}$ is a linear combination of elements of $\mathcal{H}_1$ and $\mathcal{H}_2$. Moreover $\mathcal{H}$ is the smallest vector space (a subspace of space of which $\mathcal{H}_1$ and $\mathcal{H}_2$ were subspaces of) that contains both $\mathcal{H}_1$ an $\mathcal{H}_2$. For every $x \in \mathcal{X}$ it contains the function $K_x(\cdot)$, since $K_x(\cdot) = \alpha K_{x1}(\cdot) + \beta K_{x2} (\cdot)$ where $K_{x1}(\cdot)$ is the function in $\mathcal{H}_1$ that maps each $y \in \mathcal{X}$ to $K_1(x,y)$ and $K_{2x}(\cdot)$ is defined similarly. We have, finally, that $K(x,y) = \langle K_x, K_y \rangle_{\mathcal{H}} = \langle \alpha K_{1x} + \beta K_{2x}, \alpha K_1y + \beta K_{2y} \rangle = \langle$


\subsection{Problem 2}
		Let $n \in \mathbb{N}$. Choose any $n$ data observations $\{x_i | x_i \in \mathcal{X}, i=1\ldots n\}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$. Consider the sum $$\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \langle \mathbf{\Phi}(x_i), \mathbf{\Phi}(x_j) \rangle = \langle \sum_{i=1}^n \alpha_i \mathbf{\Phi}(x_i), \sum_{j=1}^n \alpha_j \mathbf{\Phi}(x_j) \rangle = \langle s, s \rangle \geq 0,$$ where $s=\sum_{i=1}^n \alpha_i \mathbf{\Phi}(x_i)$, by the bilinearity of a (real) inner product space and by the positive-definiteness property of the inner product.






	Take the completion of the span of $\{\Psi(x) | x \in \mathcal{X}\}$. This space is therefore a complete subspace of $\mathcal{F}$ and hence a Hilbert space.

	By the Aronszajn theorem, given the positive definite kernel $K$ we can construct the unique Hilbert space of functions on $\mathcal{X}$ for which $K$ is reproducing as the completion of the functions $K_x: \mathcal{F} \rightarrow \mathbb{R} = K(\Psi(x), \cdot)$
	
%	can be constructed as the completion of all the vectors $\{\Psi(x) | x \in \mathcal{X}\}$. Let $f \in \mathcal{F}$. Then $f = \sum_i \ma

\section{Exercise 3}
\subsection{Problem 1}  
\subsubsection{$\mathcal{H}$ is a Hilbert space} First we note that $\mathcal{H}$ is a vector space. For $f_1, f_2 \in \mathcal{H}$, we see that $f = af_1 + bf_2$ will satisfy $f(0) = af_1(0)+bf_2(0) = 0$. To see that $f$ will remain absolutely continuous, we note the equivalent definition of absolutely continuous functions on a compact interal $[a,b]$: that $f$ is absolutely continuous if $f$ has derivative $f'$ almost everywhere for which $f(x) = f(0) + \int_0^1 f'(t) dt = af_1(0)+bf_2(0) + a \int_0^1 f_1'(t) dt + b \int_0^1 f_2'(t) dt = f(0) + \int_0^x [af_1'(t) + bf_2'(t)] dt$ for all $x \in [0,1]$. Here $f'(x) = af_1'(x) + bf_2'(x)$ almost everywhere, and this derivative is in $L^2([0,1])$ since $L^2([0,1])$ is a Hilbert (and in particular, vector) space.

That $\langle f, f \rangle_{\mathcal{H}}$ is a real inner product is evident since it is obviously symmetric and it is bilinear by the linearity of the integral and positive definite since $\langle f, f \rangle_{\mathcal{H}} = \int_{0}^1 (f'(x))^2 dx \geq 0$. Since by absolute continuity, the initial condition, and the Cauchy-Scharz inequality, we have that $|f(x)| = |\int_0^x f'(t)dt| = |\int_0^1 \mathbf{1}_{t\leq x}(t) f'(t) dt| \leq \sqrt{\int_0^1 f'(t)^2 dt} \cdot \sqrt{\int_0^1 \mathbf{1}_{t \leq x}(t)^2 dt}= ||f||_{\mathcal{H}} \cdot \sqrt{x}$, we see that if $f$ has zero norm it must be identically equal to 0; by absolute continuity and the initial condition only $f(x) \equiv 0$ has zero norm (since $\langle g, g \rangle_\mathcal{H} = \int_0^1 (f'(t))^2 dt = 0 \implies f'(t) = 0 \in [0,1] \implies f(t)$ is constant and we know $f(0) = 0$. Thus this space is pre-Hilbertian.

Now let $(f_n)_{n \in \mathbb{N}}$ be a Cauchy sequence in the pre-Hilbert space $\mathcal{H}$---ie, for every $\epsilon > 0$ there exists $N$ after which for all $a, b > N, ||f_a - f_b||^2_\mathcal{H} < \epsilon \implies ||f'_a - f'_b||^2_{L^2([0,1])} < \epsilon.$ Then $(f'_n)_{n \in \mathbb{N}}$ is Cauchy in $L^2([0,1])$, a complete space in $L^2([0,1])$, and thus converges to a limit $f'_L$.

Fix $x\in [0,1]$. Since $(f_n)_{n \in \mathbb{N}}$ is a Cauchy sequence of functions in $\mathcal{H}$, and since we established earlier that $|f_n(x)| \leq ||f_n||_\mathcal{H} \sqrt{x}$, we have that for all $\epsilon > 0$ there exists an $N$ such that $a,b > N \implies ||f_a - f_b||_\mathcal{H} < \epsilon$ and hence  $|f_a(x) - f_b(x)| < \epsilon \sqrt{x}$. Thus, if we want $|f_a(x) - f_b(x)| < \epsilon_0$, we choose $N_0$ such that $a,b > N_0 \implies ||f_a - f_b || \leq \frac{\epsilon_0}{\sqrt{x}}$ and hence $|f_a(x) - f_b(x)| < \frac{\epsilon_0}{\sqrt{x}}\sqrt{x} = \epsilon_0.$ Thus for all $x \in [0,1]$ we have shown that $(f_n(x))_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$ and hence convergest to a limit $f(x) \in \mathbb{R}$. Define $f_L$ as the set of pointwise limits defined in this way. We see trivially that $f_L(0)$ as the limit of functions in the pre-Hilbert space of functions each satisfying $f_n(0) = 0$ must itself be 0. Finally, consider the function $g$, defined in the previous paragraph as a function in the equivalence class of functions $f_L'$. We have, for all $x \in [0,1], f_L(x) = \lim_{n\rightarrow \infty} f_n(x) = \lim_{n\rightarrow \infty} \int_0^x f_n'(t)dt = \int_0^x \lim_{n \rightarrow \infty} f_n'(t) dt = \int_0^x f'_L(t) dt$, where $f'_L$ was defined in the previous paragraph as the $\lim_{n\rightarrow \infty} f'_n$. By the fundamental theorem of Lebesque integral calclulus we must have that the derivative of $f_L$ is $f'_L$ almost everywhere and so $f_L$ has derivative in $L^2([0,1])$. Finally, since $f_L(0) = 0$, we have that $f_L(x) = f_L(0) + \int_0^x f'_L(t) dt$ for all $x \in [0,1]$ and since $f_L'\in L^2([0,1])$, $f_L$ must be absolutely continuous.\footnote{We have abused notation slightly by refering to $f'_L$ as a function and not an equivalence class of functions but the result holds with any element of $f'_L$ so so long as we say the magic words ``almost everywhere."} Hence $\mathcal{H}$ is complete.

\subsubsection{RKHS}
The kernel associated with this RKHS is $K(x,y) = \min(x,y).$ We will show that the following is the reproducing kernel $$K_x(y) = \min(x,y) = \begin{cases} y &\mbox{if } y \leq x \\ 
x & \mbox{otherwise } \end{cases}.$$ 

We will define as its derivative $K'_x(y) = \frac{d}{dy} K_x(y) = \mathbf{1}_{y \leq x}$ (with $K'_x(x) = 1$). It is continuous and differentiable almost everywhere and hence absolutely continuous. This derivative is clearly in $L^2([0,1])$. Moreover $K_x(0) = \min(x,0) = 0$. So $K_x \in \mathcal{H}$. 

Finally we verify the reproducing properity: $f(x) = \langle f, K_x \rangle_\mathcal{H} = \int_0^1 f'(y) \mathbf{1}_{y \leq x} dy = 0+ \int_0^x f'(y) dy = f(0) + \int_0^x f'(y) dy$.

\subsection{Problem 2}
Let $\mathcal{H}_2 = \{f: [0,1] \rightarrow \mathbb{R} | f $ is absolutely continuous, $f' \in L^2([0,1]), f(0) = 0, f(1) = 0\}$. Call the Hilbert space from the previous problem $\mathcal{H}_1$. Note that the null space of the map from $\mathcal{H}_1 \rightarrow \mathbb{R}$ defined by $f \rightarrow f(1)$ is precisely $\mathcal{H}_2$. This map is clearly linear and we can show it's continuous by proving that it's bounded. Note that $||f(1)||_\mathbb{R} = |f(1)| = |\int_0^1 f'(t)dt| = |\langle f, \text{Id} \rangle_{H_1} | \leq ||f||_{\mathcal{H}_1}||\cdot ||\text{Id}||_{\mathcal{H}_1} = \frac{1}{3} ||f||_{\mathcal{H}_1}$, where we used the Cauchy-Schwarz inequality. Thus since this map from $\mathcal{H}_1 \rightarrow \mathbb{R}$ is a bounded linear operator, its nullspace is a closed subspace of $\mathcal{H}_1$. Thus $\mathcal{H}_2$ is a closed subspace of $\mathcal{H}_1$ and is itself therefore (as a closed subspace of a complete subspace and therefore a complete space) a Hilbert space (with the same inner product of the previous problem).

To satisfy the reproducing property, our reproducing kernel $K_x$ should satisfy $f(x) = \langle f, K_x \rangle_{\mathcal{H}_2} = \int_0^1 f'(t) K_x'(t) dt = [f(t)K_x'(t)]_0^1 - \int_0^1 K_x''(t)f(t)dt =  - \int_0^1 K_x''(t)f(t)dt$ given the boundary conditions $f(0)=f(1)=0$ and $K_x(0) = K_x(1) =0$.

Twice integrating $K_x''(t) = -\delta(t-x)$, using integration constants to comply with the boundary conditions $K_x(0) = K_x(1) = 0$, we get $K_x(t) = \begin{cases} (-x+1)t  &\mbox{if } t < x \\ 
-xt+x & \mbox{otherwise } \end{cases}$ for $x,t \in [0,1]$. We may rewrite this as $K(x,y) = K_x(y) = \min(x,y) - xy.$ This is differential almost everywhere and it is continuous, hence it is absolutely continuous. By construction, $K_x(0) = K_x(1) = 0$. We have that $K_x'(y) = \mathbf{1}_{x<y} -y$, which is clearly in $L^2([0,1])$. Thus $K_x \in \mathcal{H}_2$. 

We note that, on $[0,1]^2$, $K(x,y) = \min(x,y) - xy = \min(x,y) - \min(x,y)\cdot \max(x,y) = \min(x,y) \cdot (1-\max(x,y)) = \min(x,y) \cdot \min(1-x,1-y)$ is the product of two p.d. kernels.

Finally, we verify that it is a reproducing kernel:
$$\langle f, K_x \rangle_{\mathcal{H}_2} = \int_0^1 f'(t) K_x'(t) dt = \int_0^1 f'(x)\mathbf{1}_{x<t} dt - x\int_0^1f'(t)dt = \int_0^x f'(t) dt - [xf(t)]_0^1 = f(x)$$

%Letting $L$ be the second-order differential operator operating on one variable, we may rewrite this as

%$$f(x) = \int_0^1 LG(x,t)f(t)dt \iff Lf(x) = L(\int_0^1G(x,t)f(t)dt$$, suggesting that $f(x) = \int_0^1 G(x,t)f(t)dt$.

%at $f_2 \equiv 0$, for every $\epsilon > 0$, whenever $||f_1 - f_2||_{\mathcal{H}_1} < \delta = \sqrt{\epsilon}$, for some $f_1, f_2 \in \mathcal{H}_1$, we have that $||f_1 - f_2||_{\mathcal{H}_1}^2 = |\int_0^1 f_1'(t) dt - \int_0^1 f_2'(t) dt| = |f_1(1) - f_1(0) - f_2(1) + f_2(0)| = |f_1(1) - f_2(1)| < \delta^2 = \epsilon$. 

\subsection{Problem 3}
First we note that this bilinear form is obviously, well, bilinear, it's positive semi-definite ($f(u)^2 + f'(u)^2 \geq 0 \forall u \in [0,1]$) but not necessarily positive definite since it's possible that $\int_0^1 f(u)g(u) du = -\int_0^1 f'(u)g'(u)du$ for $f \neq g$).

We want to find a kernel $K$ so that the reproducing property to hold: $f(x) = \langle f, K_x \rangle_\mathcal{H} =  \int_0^1 f(u)K_x(u)du + \int_0^1 f'(u)K_x'(u)du$, where since $f, K_x \in \mathcal{H}, f(0) = f(1) = 1$. We rewrite this using integration by parts and the boundary condition:
$$f(x) = \langle f, K_x \rangle_\mathcal{H} = \int_0^1 f(u)[K_x(u) - K_x''(u)]du$$.

In other words, we want $K_x(u) - K_x''(u) = \delta(u-x)$ such that $K_x(0) = K_x(1) = 0$. %I've solved this and frankly it's hell just to type it in LaTex so I will try to think of another way.

Taking the Laplace transform of both sides (noting the initial condition $K_x(0)=0$), solving for $K_x(s)$, using partial fraction decomposition to find inverse Laplace transforms, and again noting both initial conditions and finally simplifying the answer by removing terms that are zero on the domain $[0,1]$, we get the following:

$$K_x(u) = \frac{1}{2(e+1)(e-1)}e^{-(u+x)} \cdot $$ \\
$$\bigg(-(e^u+1)(e^u-1) -(e+1)(e-1)(e^{2u}-e^{2x})\mathbf{1}_{u\geq x} + (e^{2u}-e^2)(e^x + 1)(e^x-1))\mathbf{1}_{x=0}\bigg)$$.

This should have the reproducing function and boundary conditions by construction. It is trivial to verify symmetry.

We can factor this mess into hyperbolic sinusoids but first let's look at the homogeneous equation $K_x(u) - K_x''(u) = 0$. It has solutions like $e^x$ or $e^{-x}$. Since scaling an exponential is equivalent to shifting it these can be delayed or advanced. Let $K_x(u) = a \sinh u + b \cosh u$. A solution that respects our first boundary condition is $K_x(u) = \sinh u$ and one that respects our second condition is $K_x(u) = \sinh (u-1)$. We're looking for a solution, then, of $K_x(u) = \alpha \sinh u + \beta \sinh (u-1)$ so that $K_x(0) = K_x(1) = 0$. We also wish for the derivative at $x$ from the left and from the right

%First we let $K_1(f,g) = \int_0^1 f(u)g(u) du$ and $K_2(f,g) = \int_0^1 f'(u)g'(u) du$. $K_1$ is a kernel %but $K_2$ is merely a bilinear form. In other words, $K_1$ is positive definite and $K_2$ is positive definite. But $K(f,g)$ is necessarily positive definite because given functions $f_1 \ldots f_n \in \mathcal{H}$ their Gram matrix is positive definite 
%since if we have $\sum_{i=1}^n \alpha_i\sum_{j=1}^n \alpha_j \int_0^1f_i(u)f_j(u)du = \int_0^1 \big(\sum_{i=1}^n \alpha_i f_i(u) \big) \big(\sum_{j=1}^n \alpha_j f_j(u)\big) du = \int_0^1 \big(\sum_{i=1}^n \alpha_i f(u)\big)^2 du \geq 0$. By an identical argument we have that $K_2(f,g)$ is a positive definite kernel even though it is a bilinear form and not a proper inner product. Hence $K(f,g) = K_1(f,g) + K_2(f,g)$, as a nonnegative-weighted sum of positive definite kernels, is itself a positive definite kernel.

%Since $K_1(f,g)$ is already a 


\section{Exercise 4}
\subsection{1}
We use the representer theorem. We have the constraint $||f||_{\mathcal{H}_K} \leq B$. Consider some solution $f$ which can be represented as the sum of its projection $f^S$ onto span$\{K_{x_1},\ldots, K_{x_n}\}$ and its projection onto the orthogonal complement $f^\bot$. By the reproducing property, for each $i=1\ldots n$, $f(x_i) = \langle f, K_{x_i} \rangle_\mathcal{H} = \langle f^S, K_{x_i} \langle_\mathcal{H} = f^S(x)$ since $K_{x_i}$ is in span$\{x_1, \ldots, x_n\}$ Since $||f^S|| \leq ||f||$. Thus for any function $f$ that minimizes the objective, projecting that $f$ onto this span won't increase the value of the objective and the constraint will continue to hold.

If $f \in $ span$\{K_{x_1},\ldots, K_{x_n}\}, f = \sum_{i=1}^n \alpha_i K_{x_i}$, and so $||f||_\mathcal{H}^2 = \sqrt{\alpha^T K \alpha}$, where $K$ is the Gram matrix of the kernel on the data $\{x_1, \ldots, x_n \}$ and by the positive definiteness of the kernel $K$, it has a square root, and so $||f||_\mathcal{H} = K^{1/2}\alpha$. Since $f(x_j) = \sum_{i=1}^n \alpha_i K(x_j, x_i) = [K\alpha]_j,$ where $[K \alpha]_j$ indicates the $jth$ element of the $n$-by-1 vector $K\alpha$ and $\alpha = (\alpha_1, \ldots, \alpha_n)^T$.

Let the function $R(f) = \frac{1}{n} \sum l_{y_i}(f(x_i)) = \frac{1}{n} \sum_{i=1}^n l_{y_i}([K\alpha]_i)$ map a function $f\in \mathcal{H}$ to its risk, informed by the data. Since $f \in $ span$\{K_{x_1},\ldots, K_{x_n}\}$ as noted above, $f = K\alpha$ and we can therefore rewrite our empirical risk $R$ as follows: $R(K\alpha)= \frac{1}{n} \sum_{i=1}^n l_{y_i}([K\alpha]_i)$. Given that the loss functions are convex, and we are taking a positive-scaled conic combination of them, we see that we have a convex optimization problem with a sublinear inequality constraint: $\min_{\alpha \in \mathbb{R}^n} R(K\alpha)$ subject to $||f||_\mathcal{H} = K^{1/2}\alpha \leq B$ or $||f||_\mathcal{H}^2 = \alpha^T K \alpha \leq B^2.$

We can take the Lagrangian:
$\mathcal{L}(\alpha, \lambda) = R(K\alpha) + \lambda(\alpha^TK\alpha - B^2)$ subject to $\lambda \geq 0$. 

We can get the dual problem by solving taking $\inf_{\alpha \in \mathbb{R}^n} R(K\alpha) + \lambda(\alpha^T K \alpha - B^2) = \inf_{\alpha \in \mathbb{R}^n} R(K\alpha) + \lambda(\alpha^T K \alpha)$ since we need only the terms that involve $\alpha$. Let $\alpha^*$ be the solution to this problem.

We note that 0 is strictly feasible since $B > 0$ and so by Slater's conditions strong duality holds.

Our dual problem is therefore $\max R(K\alpha^*) + \lambda((\alpha^*)^T K \alpha^* - B^2)$ subject to $\lambda \geq 0$. It is linear in $\lambda$ and has negative or zero slope by the constraint given in the initial problem. (The slope is $(\alpha^*)^T K \alpha^* - B^2)$.) In either case, the maximum value will occur with $\lambda = 0$. Thus our minimimum risk will be the dual solution: $R(K\alpha^*)$.

Our algorithm for solving (1) is therefore to solve (2) to find $\alpha*$ and plug $\alpha^*$ into $R(K\alpha^*)$ to get teh answer.

\subsection{2}
\begin{align*} R^*(u) &= \sup_{x \in \mathbb{R}^n} x^t u - \frac{1}{n} \sum_{i=1}^n l_{y_i}(x_i) \\
	& = \sup_{x \in \mathbb{R}^n} \sum_{i=1}^n x_i u_i - \frac{1}{n} \sum_{i=1}^n l_{y_i}(x_i) \\
	& = \frac{1}{n} \sum_{i=1}^n \sup_{x_i \in \mathbb{R}} nx_i u_i  - l_{y_i}(x_i) \\
	& = \frac{1}{n} \sum_{i=1}^n \sup_{x_i\in \mathbb{R}} x_i \cdot (nu_i) - l_{y_i}(x_i) \\
	& =  \frac{1}{n} \sum_{i=1}^n l^*_{y_i}(nu_i).
\end{align*}
\subsection{3}
The dual problem can be written, taking the Langrangian with the equality constraint, as the following:

$\max_{\mu} \min_{\alpha, u} (R(u) + \lambda \alpha^T K \alpha + \mu^T(K\alpha -u))$ subject to $\mu \succeq 0$. We solve the two minimization problems separately. The first, by pattern-matching, we see is $R^*(\mu)$. The second is a quadratic form and since $K \succ 0$ it is convex. Taking the gradient with respect to $\alpha$ and setting the result to zero, we get $2 \lambda K\alpha + K^T \mu = 0$, from which point we note that $K$ is symmetric and get $ \lambda K \alpha = -\frac{1}{2} K \mu$. Since $K$ is invertible this means $\alpha = -\frac{1}{2 \lambda} \mu$ at the minimum.

Hence we may rewrite the dual problem as $\max_{\mu} -R^*(\mu) -\frac{1}{4\lambda} \mu^T K \mu$ subject to $\mu \succeq 0$.

Since (3) is convex, (it is positive sum of a convex function $R(u)$ and a convex quadratic (since $K \succ 0$ subject to a convex (linear) equality constraint). We can again apply Slater's condition to note that strong duality holds (there is only a single constraint in the primal, a linear equality constraint, that is feasible with $K$ being positive definite and hence invertible) and thus the solution to the dual problem will equal the solution to the primal problem.


\subsection{4}
We wish to find $f^*(u) = \sup_x (xu - \max(0, 1-yx)^2)$. First let $y=-1$. We note that $\max(0, 1+x) = 0$ if $x\leq -1$ and $1+x$ otherwise. Thus, if $\sup_x (xu-\max(0,1+x))$ happens where $x \leq -1$, we have that $f^*(u) = xu$ and if $\sup_x (xu-\max(0,1+x))$ happens where $x \geq -1$, we have (by the first order condition and the concavity of the expression) that $f^*(u) = u(\frac{u}{4} -1)$ and is attained where $x = \frac{u}{2} -1$. Thus, if $\frac{u}{2} -1 \leq -1 \iff u\leq 0$ we will be in the former case and if not we'll be in the latter case. Hence, we have that $f^*(u) = \infty $ if $u \leq 0$ and $u(\frac{u}{4}-1)$ if not. Now let's consider the case where $y=+1$. By the same argument above, we have that $f^*(u) = \infty$ when $u \leq 0$ and $u(\frac{u}{4} + 1)$ otherwise. We may thereby write $f^*(u) = l_{y-hinge}^*(u) = \infty$ if $u\leq 0$ and $u(\frac{u}{4} + y)$.

We wish to find $f^*(u) = \sup_x(xu - \log(1+e^{-yx})$. We note that this is a linear function in x minus a convex function and is thus concave. We apply the first order conditions and see that this must occur when $u = \frac{-y e^{-yx}}{1+e^{-yx}}  = -y(1-\textnormal{sigmoid}(yx))$ or $x = \frac{1}{y} \log (\frac{u+y}{1-u-y})$ In this case $f^*(u) = \frac{u}{y}\log (\frac{u+y}{1-u-y}) - \log \big(1+ e^{-y\log \big(\frac{u+y}{1-u-y}\big)}\big) = \frac{u}{y} \log(\frac{u+y}{1-u-y}) - \log (\frac{1}{u+y})$, which is defined for $u \in (-y, 1-y)$. 



\end{document}
