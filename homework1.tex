% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework}%replace X with the appropriate number
\author{Max Dunitz\\ %replace with your name
Kernel Methods} %if necessary, replace with your course title
 
\maketitle


\begin{exercise}{1} 
	\begin{enumerate}
		\item The symmetry of the kernel $K(x,y) = \cos (x-y) = \cos (y-x) = K(y,x)$ follows immediately from the evenness of the cosine function. The positive definiteness can be seen by noting that $K(x,y) = \cos(x-y) = \frac{1}{2} e^{j(x-y)} + \frac{1}{2} e^{-j(x-y)}$, a nonnegative linear combination of two exponential kernels. The positive definiteness of the latter is established by the following: For any real collection of $\alpha_i$, $\sum_i \sum_k \alpha_i \alpha_k e^{j(x_i - y_k)} = \big(\sum_i \alpha_i e^{jx_i}\big)\big(\sum_k \alpha_k e^{-j x_i} \big).$ Letting $s=\sum_i \alpha_i e^{jx_i}$, this becomes $s \cdot \bar{s} = |s|^2 \geq 0$, thus establishing the positive definiteness of $K_{exp} = e^{j(x-y)}$. The argument for the positive definiteness of $K_{exp-conjugate} = e^{-j(x-y)}$ holds by symmetry. Thus this cosine kernel, as the nonnegative combination of two positive definite kernels, is positive definite.
		\item The symmetry of the kernel $K(x,y) = \frac{1}{1-x^Ty} = \frac{1}{1-y^Tx} = K(y,x)$ follows immediately from the symmetry of the linear kernel $K_{lin}(x,y) = x^Ty$. Since $\mathcal{X}$ contains only those vectors $x \in \mathbb{R}^p$ for which $||x||_2 < 1$, by the Cauchy-Schwartz inequality we have that for $x,y \in \mathcal{X}, |x^T y| < 1$ and so $K(x,y)$ may be expressed as the convergent infinite series $1 + (x^T y) + (x^T y)^2 + (x^T y)^3 + \ldots.$ We can thus define a sequence of positive definite kernels $K_i(x,y) = \sum_{k=0}^i (x^Ty)^k$ for $i >0$ that converges pointwise to $K(x,y) = \frac{1}{1-x^Ty}$. Each partial sum in the sequence is positive definite as it is a non-weighted sum of positive definite (polynomial) kernels. But there is a slight complication: the zeroth-order kernel $K(x,y) = 1$ is only positive semidefinite, not positive definite. That is, the rank-one Gram matrix associated with the symmetric ``semidefinite kernel" $K(x,y) = 1$---ie, the all-ones matrix---has one positive eigenvalue but the rest are zero. This is not a concern as the sum of a positive definite matrix with that of a positive semidefinite matrix is positive definite: if $x^TAx > 0$ and $x^TBx \geq 0$ then $x^T(A+B)x = x^TAx + x^T B x > 0.$
		\item Since $P(A \cap B) = P(B \cap A)$ and $P(A)P(B) = P(B)P(A)$, the kernel is symmetric. Given a set $A$ define the indicator function $I_A: \mathbb{R} \rightarrow \mathbb{R}$ that maps $x$ to 1 if $x\in A$ and 0 otherwise. This function is measurable when $A$ is measurable with respect to a probability measure. Let $\mu_A \triangleq \mathbb{E}[I_A] = \int I_A (x) d \mu (x) = \int_{x \in A} d \mu (x) = P(A).$ Now define the function $\phi_A(x) = I_A(x) - \mu_A.$ We can repeat this process, making equivalent definitions of indicator function, mean, and difference function $\phi$, for any set---and use them to rewrite our kernel as an inner product: $K(A,B) = P(A\cap B) - P(A)P(B) = \int I_A (x) I_B(x) d \mu(x) - \big(\int I_A(x) d \mu(x) \big) \cdot \big(\int I_B(x) d\mu(x)\big) = \mathbb{E}[I_A I_B] - \mathbb{E}[I_A]\mathbb{E}[I_B] =\mathbb{E}[I_AI_B] - \mu_A\mu_B = \mathbb{E}[I_AI_B] - \mu_A\mathbb{E}[I_B] - \mu_B \mathbb{E}[I_A] -\mu_A\mu_B = \mathbb{E}[(I_A -\mu_A)(I_B - \mu_B)] = \int \phi_A (x) \phi_B (x) d \mu (x) = \langle \phi_A, \phi_B \rangle_{L_2(\mu)}$. Having found an appropriate inner product, we can apply the Aronsajn theorem, thereby establishing the positive definiteness of $K(A,B)$. We note that for all $x\in \mathbb{R}$, we have that $I_A(x)I_B(x) \leq I_A(x)$ and $\int I_A(x) \cdot \mu_B d\mu(x) = \mu_B \cdot \int I_A(x) d\mu(x)$ is finite since $A$ an $B$ are measurable sets (and hence $\mu_A$ and $\mu_B$ are finite), this is all good and kosher.
		\item We note that $K_4(x,y) = \min \big(f(x)g(y), f(y)g(x)\big) = \min \big(f(y)g(x), f(x)g(y)\big) = K_4(y,x)$, where the second equality holds by the symmetry of the min function. Thus $K_4$ is symmetric. To see that this is a positive definite kernel, let us first factor out $f(x)f(y)$ to rewrite $K_4(x,y) = f(x)f(y)\min\big(\frac{g(x)}{f(x)}, \frac{g(y)}{f(y)}\big)$. We can do this since $f$ is positive. Now let the function $\mathbf{1}_a$ map reals between 0 and $a$ to 1 and all other reals to 0. As we saw in class, $K_a(x,y) = \min\big(\frac{g(x)}{f(x)}, \frac{g(y)}{f(y)}\big)$ is a positive definite kernel, which we can see by finding an inner product and applying Aronszajn's theorem or directly from the definition: since for any choice of $n$ vectors $x_1, \ldots , x_n$ in $\mathcal{X}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$, we have that 
			\begin{align*} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \min\bigg(\frac{g(x_i)}{f(x_i)}, \frac{g(x_j)}{f(x_j)}\bigg) &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_{0}^\infty \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \mathbf{1}_{\frac{g(x_j)}{f(x_j)}}\\ &= \int_{0}^\infty  \bigg(\sum_{i=1}^n \alpha_i \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \bigg) \bigg(\sum_{j=1}^n \alpha_j \mathbf{1}_{\frac{g(x_j)}{f(x_j)}}\bigg)\\ &=  \int_{0}^\infty  \big(\sum_{i=1}^n \alpha_i \mathbf{1}_{\frac{g(x_i)}{f(x_i)}} \big)^2 \geq 0,\end{align*} we see that $K_a$ is positive definite. Similarly, $K_b(x,y) = f(x)f(y)$ is positive definite since for any choice of $n$ vectors $x_1, \ldots , x_n$ in $\mathcal{X}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$, we have that \begin{align*}\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j f(x_i)f(x_j) &= \bigg(\sum_{i=1}^n \alpha_i f(x_i)\bigg)^2 \geq 0.\end{align*} Since $K_4(x,y) = K_a(x,y)K_b(x,y)$ is the product of two positive definite kernels it must itself be positive definite. (See slide 48 for proof.)
			\item The symmetry of the kernel follows immediately from the facts that $A \cup B = B \cup A$ and $A \cap B = B \cap A$. Note that $|A \cap B| = \sum_{x\in E} I_x(A) \cdot I_x(B)$, where the indicator $I_x(C): \mathcal{P} \rightarrow \mathbb{R}$ takes the value 1 if $x \in C$ and 0 otherwise. We may also write $|A \cup B| = |E| - |\bar{A} \cap \bar{B}|,$ where for any set $C$, $\bar{C}$ indicates its set complement. We will let $K_1(A,B) = |A \cap B|$ and $K_2(A,B) = \frac{1}{|A \cup B|} = \frac{1}{|E| - |\bar{A} \cap \bar{B}|}$ and show that $K(A,B) = K_1(A,B) \cdot K_2(A,B)$ is the product of two positive definite kernels. Let us index the $n=|E|$ elements of the set $E$ as follows: $\{x_i | i \in I \}$  with the index set $I=\{1,\ldots, n\}$. Consider the feature map $\phi(C): \mathcal{P}{E} \rightarrow \mathbb{R}^{n} = (I_{x_1}(C), \ldots, I_{x_n}(C))^T$. Then $K_1(A,B) = \langle \phi(A), \phi(B) \rangle$ where the inner product used is the standard inner product used for $\mathbb{R}$. Hence by Aronszajn's theorem it is positive definite. Now let $\psi(C): \mathcal{P}(E) \rightarrow \mathbb{R}^{n} = \frac{1}{\sqrt{n}}\cdot (\bar{I}_{x_1}(C), \ldots, \bar{I}_{x_n}(C))^T,$ where $\bar{I}_{x}(C): \mathcal{P}(E) \rightarrow \mathbb{R}$ takes the value 0 if $x \in C$ and 1 otherwise. Then $K_2(A,B) = \frac{1}{n - n \psi(A)^T\psi(B)} = \frac{1}{n} \cdot \frac{1}{1- \psi(A)^T\psi(B)}$ and we can apply Aronszajn's theorem, exercise 1-2, and the property that nonnegative scalar multiples of positie definite kernels are positive definite (see slide 46). Thus $K$ is the product of two positie definite kernels and is positive definite by the result shown in class.
				
				%We saw in class that $K_{min-over-max}(x,y) = \frac{\min(x,y)}{\max(x,y)}$ is a positive definite kernel (it is the product of the two positive definite kernels $\min(x,y)$ and $\big(\frac{1}{\min(1/x, 1/y)}\big)$. Let $I_A(x)$ be the indicator for $A$ (ie, it equals 1 if $x \in A$ and 0 otherwise) and $I_B(x)$ the indicator for $B$. Let $f_x$ map a set $BQH$.Then $K(A,B) = \sum_{x \in E} \frac{|A \cap B|}{|A \cup B|} = \sum_{x \in E} \frac{\min(I_A(x), I_B(x))}{\max(I_A(x), I_B(x))}$ is a nonnegative weighted sum of positive definite kernels and is therefore positive definite.
	\end{enumerate}
\end{exercise}

\begin{exercise}{2}
	\begin{enumerate}
		\item Let $K \triangleq \alpha K_1 + \beta K_2$. That $K$ is symmetric follows immediately from the symmetry of $K_1$ and $K_2$: for some $x, y \in \mathcal{X}, K(x,y) = \alpha K_1(x,y) + \beta K_2(x,y) = \alpha K_1(y,x) + \beta K_2(y,x) = K(y,x)$. That $K$ is positive definite comes almost as easily: for any set of data observations $\{x_i | x_i \in \mathcal{X}, i=1,\ldots,n\}$ and corresponding real weights $\alpha_i$, we have that for $K_j \in \{K_1, K_2 \}, \sum_{i=1}^n \sum_{l=1}^n \alpha_i \alpha_l K_j(x_i, x_l) \geq 0$---and since $\alpha, \beta > 0$, we see that $\sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K(x_i, x_l) = \sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l \alpha K_1(x_i, x_l) + \beta K_2(x_i, x_l) = \alpha \cdot \big( \sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K_1(x_i, x_l)\big) + \beta \cdot \big(\sum_{i=1}^n \sum_{=1}^n \alpha_i \alpha_l K_2(x_i, x_l)\big) \geq 0$. Thus $K$ is positive definite.

			We now consider what $K$'s RKHS is. If $\mathcal{H}_1$ is the RKHS associated with $K_1$ an $\mathcal{H}_2$ the RKHS associated with $K_2$ are both subspaces of the same Hilbert space, over the same field $\mathcal{F}$ with the same norm $|| \cdot ||$, define $\mathcal{H} \triangleq \mathcal{H}_1 + \mathcal{H}_2 = \{a_1 x_1 + a_2 x_2 | x_1 \in \mathcal{H}_1, x_2 \in \mathcal{H}_2, a_1,a_2 \in \mathcal{F}\}$. Clearly any sequence in $\mathcal{H}$ that is Cauchy with respect to the norm $|| \cdot ||_\mathcal{H}$  will converge since each element of $\mathcal{H}$ is a linear combination of elements of $\mathcal{H}_1$ and $\mathcal{H}_2$. Moreover $\mathcal{H}$ is the smallest vector space (a subspace of space of which $\mathcal{H}_1$ and $\mathcal{H}_2$ were subspaces of) that contains both $\mathcal{H}_1$ an $\mathcal{H}_2$. For every $x \in \mathcal{X}$ it contains the function $K_x(\cdot)$, since $K_x(\cdot) = \alpha K_{x1}(\cdot) + \beta K_{x2} (\cdot)$ where $K_{x1}(\cdot)$ is the function in $\mathcal{H}_1$ that maps each $y \in \mathcal{X}$ to $K_1(x,y)$ and $K_{2x}(\cdot)$ is defined similarly. We have, finally, that $K(x,y) = \langle K_x, K_y \rangle_{\mathcal{H}} = \langle \alpha K_{1x} + \beta K_{2x}, \alpha K_1y + \beta K_{2y} \rangle = \langle$

		\item Let $n \in \mathbb{N}$. Choose any $n$ data observations $\{x_i | x_i \in \mathcal{X}, i=1\ldots n\}$ and corresponding real weights $\alpha_1, \ldots, \alpha_n$. Consider the sum $\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \langle \mathbf{\Phi}(x_i), \mathbf{\Phi}(x_j) \rangle = \langle \sum_{i=1}^n \alpha_i \mathbf{\Phi}(x_i), \sum_{j=1}^n \alpha_j \mathbf{\Phi}(x_j) \rangle = \langle s, s \rangle \geq 0,$ where $s=\sum_{i=1}^n \alpha_i \mathbf{\Phi}(x_i)$, by the bilinearity of a (real) inner product space and by the positive-definiteness property of the inner product.
	\end{enumerate}

\end{exercise}

\section{Exercise 3}
\subsection{Problem 1}  
\subsubsection{$\mathcal{H}$ is a Hilbert space} First we note that $\mathcal{H}$ is a vector space. For $f_1, f_2 \in \mathcal{H}$, we see that $f = af_1 + bf_2$ will satisfy $f(0) = af_1(0)+bf_2(0) = 0$. To see that $f$ will remain absolutely continuous, we note the equivalent definition of absolutely continuous functions on a compact interal $[a,b]$: that $f$ is absolutely continuous if $f$ has derivative $f'$ almost everywhere for which $f(x) = f(0) + \int_0^1 f'(t) dt = af_1(0)+bf_2(0) + a \int_0^1 f_1'(t) dt + b \int_0^1 f_2'(t) dt = f(0) + \int_0^x [af_1'(t) + bf_2'(t)] dt$ for all $x \in [0,1]$. Here $f'(x) = af_1'(x) + bf_2'(x)$ almost everywhere, and this derivative is in $L^2([0,1])$ since $L^2([0,1])$ is a Hilbert (and in particular, vector) space.

That $\langle f, f \rangle_{\mathcal{H}}$ is a real inner product is evident since it is obviously symmetric and it is bilinear by the linearity of the integral and positive definite since $\langle f, f \rangle_{\mathcal{H}} = \int_{0}^1 (f'(x))^2 dx \geq 0$. Since by absolute continuity, the initial condition, and the Cauchy-Scharz inequality, we have that $|f(x)| = |\int_0^x f'(t)dt| = |\int_0^1 \mathbf{1}_{t\leq x}(t) f'(t) dt| \leq \sqrt{\int_0^1 f'(t)^2 dt} \cdot \sqrt{\int_0^1 \mathbf{1}_{t \leq x}(t)^2 dt}= ||f||_{\mathcal{H}} \cdot \sqrt{x}$, we see that if $f$ has zero norm it must be identically equal to 0; by absolute continuity and the initial condition only $f(x) \equiv 0$ has zero norm (since $\langle g, g \rangle_\mathcal{H} = \int_0^1 (f'(t))^2 dt = 0 \implies f'(t) = 0 \in [0,1] \implies f(t)$ is constant and we know $f(0) = 0$. Thus this space is pre-Hilbertian.

Now let $(f_n)_{n \in \mathbb{N}}$ be a Cauchy sequence in the pre-Hilbert space $\mathcal{H}$---ie, for every $\epsilon > 0$ there exists $N$ after which for all $a, b > N, ||f_a - f_b||^2_\mathcal{H} < \epsilon \implies ||f'_a - f'_b||^2_{L^2([0,1])} < \epsilon.$ Then $(f'_n)_{n \in \mathbb{N}}$ is Cauchy in $L^2([0,1])$, a complete space in $L^2([0,1])$, and thus converges to a limit $f'_L$.

Fix $x\in [0,1]$. Since $(f_n)_{n \in \mathbb{N}}$ is a Cauchy sequence of functions in $\mathcal{H}$, and since we established earlier that $|f_n(x)| \leq ||f_n||_\mathcal{H} \sqrt{x}$, we have that for all $\epsilon > 0$ there exists an $N$ such that $a,b > N \implies ||f_a - f_b||_\mathcal{H} < \epsilon$ and hence  $|f_a(x) - f_b(x)| < \epsilon \sqrt{x}$. Thus, if we want $|f_a(x) - f_b(x)| < \epsilon_0$, we choose $N_0$ such that $a,b > N_0 \implies ||f_a - f_b || \leq \frac{\epsilon_0}{\sqrt{x}}$ and hence $|f_a(x) - f_b(x)| < \frac{\epsilon_0}{\sqrt{x}}\sqrt{x} = \epsilon_0.$ Thus for all $x \in [0,1]$ we have shown that $(f_n(x))_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$ and hence convergest to a limit $f(x) \in \mathbb{R}$. Define $f_L$ as the set of pointwise limits defined in this way. We see trivially that $f_L(0)$ as the limit of functions in the pre-Hilbert space of functions each satisfying $f_n(0) = 0$ must itself be 0. Finally, consider the function $g$, defined in the previous paragraph as a function in the equivalence class of functions $f_L'$. We have, for all $x \in [0,1], f_L(x) = \lim_{n\rightarrow \infty} f_n(x) = \lim_{n\rightarrow \infty} \int_0^x f_n'(t)dt = \int_0^x \lim_{n \rightarrow \infty} f_n'(t) dt = \int_0^x f'_L(t) dt$, where $f'_L$ was defined in the previous paragraph as the $\lim_{n\rightarrow \infty} f'_n$. By the fundamental theorem of Lebesque integral calclulus we must have that the derivative of $f_L$ is $f'_L$ almost everywhere and so $f_L$ has derivative in $L^2([0,1])$. Finally, since $f_L(0) = 0$, we have that $f_L(x) = f_L(0) + \int_0^x f'_L(t) dt$ for all $x \in [0,1]$ and since $f_L'\in L^2([0,1])$, $f_L$ must be absolutely continuous.\footnote{We have abused notation slightly by refering to $f'_L$ as a function and not an equivalence class of functions but the result holds with any element of $f'_L$ so so long as we say the magic words ``almost everywhere."} Hence $\mathcal{H}$ is complete.

\subsubsection{RKHS}



\section{Exercise 4}
\subsection{4}
We wish to find $f^*(u) = \sup_x (xu - \max(0, 1-yx)^2)$. First let $y=-1$. We note that $\max(0, 1+x) = 0$ if $x\leq -1$ and $1+x$ otherwise. Thus, if $\sup_x (xu-\max(0,1+x))$ happens where $x \leq -1$, we have that $f^*(u) = xu$ and if $\sup_x (xu-\max(0,1+x))$ happens where $x \geq -1$, we have (by the first order condition and the concavity of the expression) that $f^*(u) = u(\frac{u}{4} -1)$ and is attained where $x = \frac{u}{2} -1$. Thus, if $\frac{u}{2} -1 \leq -1 \iff u\leq 0$ we will be in the former case and if not we'll be in the latter case. Hence, we have that $f^*(u) = \infty $ if $u \leq 0$ and $u(\frac{u}{4}-1)$ if not. Now let's consider the case where $y=+1$. By the same argument above, we have that $f^*(u) = \infty$ when $u \leq 0$ and $u(\frac{u}{4} + 1)$ otherwise. We may thereby write $f^*(u) = l_{y-hinge}^*(u) = \infty$ if $u\leq 0$ and $u(\frac{u}{4} + y)$.

We wish to find $f^*(u) = \sup_x(xu - \log(1+e^{-yx})$. We note that this is a linear function in x minus a convex function and is thus concave. We apply the first order conditions and see that this must occur when $u = \frac{-y e^{-yx}}{1+e^{-yx}}  = -y(1-\textnormal{sigmoid}(yx))$ or $x = \frac{1}{y} \log (\frac{u+y}{1-u-y})$ In this case $f^*(u) = \frac{u}{y}\log (\frac{u+y}{1-u-y}) - \log \big(1+ e^{-y\log \big(\frac{u+y}{1-u-y}\big)}\big) = \frac{u}{y} \log(\frac{u+y}{1-u-y}) - \log (\frac{1}{u+y})$, which is defined for $u \in (-y, 1-y)$. 


\end{document}
